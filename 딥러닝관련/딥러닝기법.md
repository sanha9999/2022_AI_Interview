## 차원의 저주란 무엇인가?

차원의 저주는 한 샘플을 특정짓기 위해 많은 정보(다양한 차원의)를 수집할수록 오히려 데이터 사이의 거리가 멀어져 차원에 빈공간이 생기기 때문에 학습이 어려워지는 문제이다. 해결방법은 데이터의 밀도가 높아질때까지 데이터를 모아 훈련 세트를 키우거나, PCA같은 차원 축소 기법을 이용하여 해결한다.

## PCA기법은 무엇인가?

고차원의 데이터를 저차원의 데이터로 축소시키는 차원 축소 방법중 하나로, 훈련 데이터의 많은 feature중 중요한 feature 몇개만 뽑아내는 방법이 PCA이다. [블로그](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-9-PCA-Principal-Components-Analysis)에서 잘 정리되어있다.

## 하이퍼 파라미터는 무엇인가?

하이퍼 파라미터는 모델의 학습에 필요한 수동 설정값이다.

## Non-Linearity라는 말의 의미와 그 필요성은?

Non-Linearity는 비 선형성이라는 뜻이다. 데이터의 복잡도가 높아지고 차원이 높아지게 되면 데이터의 분포는 단순한 선형이 아닌 비 선형 형태를 가지기 때문에 단순한 1차식의 형태로는 데이터를 표현할 수 없기에 Non-Linearity가 중요하다.

## 활성화 함수(Activation Function)의 종류

- Sigmoid : 역전파 과정에서 기울기가 소실되는 문제 발생
- Tanh : Sigmoid의 기울기 소실 문제를 해결함
- ReLU : 음수일때 기울기가 0이 되버림
- Leaky ReLU : 음수에서 기울기가 0이 되는 ReLU 문제 해결
- Softmax : 다중 클래스 분류 시 사용함

## Sigmoid보다 ReLU를 많이 쓰는 이유는?

Sigmoid의 문제점으로는 입력 값이 일정 범위를 넘어가게 되면 0또는 1로 수렴해버리고 gradient가 0으로 수렴해 버리게 되어 학습이 제대로 되지 않는다. 또한 Sigmoid는 범위가 0에서 1이기 때문에 중앙값이 0이 아니게 된다. 0을 기준으로 데이터가 분포하게 되었을 때가 이상적인데 Sigmoid는 그것을 만족시키지 못한다. 또한 ReLU보다 연산에 많은 cost가 소모된다.

## ReLU는 그래프만 보면 곡선이 아닌 것 같은데 어떻게 Non-Linearity인가?

ReLU를 보면 단순 Linear 형태인 듯 보이지만, Multi-layer의 activation function으로 ReLU를 사용하게 되면 Linear 한 부분 부분의 결합의 합성 함수가 만들어 지게 되는데, 이 결합 구간을 보았을 때 최종적으로 Non-Linearity한 성질을 가지게 된다.

## ReLU의 문제점은?

ReLU의 문제점은 원래 ReLU가 max(0, x)이기 때문에 0보다 작으면 함수 미분값이 0이된다는 약점이 있다. 이 문제를 해결하기 위해 LeakyReLU라는 함수가 만들어졌다.

## Bias는 뭘까?

Bias는 모델이 데이터에 잘 fitting하게 하기 위하여 평행 이동하는 역할을 한다. 데이터를 2차원으로 표현했을 때, 모든 데이터가 원점기준에 분포해 있지는 않기 때문에 Bias를 이용하여 모델이 평면 상에서 이동할 수 있도록 하고, 이 Bias또한 학습하게 한다.

## Variance란 무엇일까?

머신러닝에서 추정값들의 흩어진 정도를 Variance라고 한다.

## Back Propagation이란?

신경망의 최종 단계에서 계산된 오차를 바탕으로 이전 단계의 파라미터를 업데이트하는 방법이다.

## Batch Normalization이란?

Batch Normalization, 즉 배치 정규화는 평균과 분산을 조정하는 과정이 별도의 과정으로 떼어진 것이 아닌 신경망 안에 포함되어 학습시 평균과 분산을 조정한다. 즉 각 레이어마다 정규화하는 레이어를 두는 것이 배치 정규화이다.

## Batch Normalization은 cnn에만 쓰이는가?

그렇다. 배치별로 계산되기 때문에 네트워크의 순환부분을 고려하지 않기 때문에 RNN이나 LSTM에서는 쓸 수 없다.

## Data Normalization은 무엇인가?

Data Normalization은 입력 데이터의 최소, 최대값을 일정 범위(0~1) 내로 조절하는 것이다. 이를 통해 특정 데이터가 결과에 대해 과도한 영향을 미칠 수 있는 지위를 획득하는 것을 방지할 수 있고, 모델의 학습을 원할하게 만든다.

## Weight Initialization이란 무엇인가?

Weight Initialization은 가중치 초기화라는 뜻으로, 딥러닝 학습에 있어 초기 가중치 설정은 매우 종요한 역할을 한다. 가중치를 잘못 설정할 경우 기울기 소실 문제나 표현력의 한계를 갖는 등 여러 문제를 야기할 수 있기 때문이다. Weight Initialization을 통해 Local minimum에 빠지는 문제를 해결할 수 있다.

## Regularization이란 무엇인가?

단순히 Cost function의 값이 작아지는 방향으로 모델을 학습하면 특정 가중치가 과도하게 커지는 현상이 나타날 수 있기 때문에 Cost function을 계산할 때 가중치의 절대값 만큼을 더해주는 방법이다. Regularization을 통해 특정 가중치의 값이 너무 커지는 현상을 억제한다.

## 딥러닝을 할 때 GPU를 쓰는 이유는?

GPU에는 부동소수점 계산에 특화된 수많은 코어가 있어서 병렬 처리를 수행하기에 유리하기 때문이다.

## 학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는 무엇일까?

GPU를 100% 활용하지 못하는 경우는 두가지로 나뉘는데, 첫번째로는 GPU 메모리를 충분히 활용하지 못하는 경우이고 두분째는 GPU의 연산 능력을 충분히 활용하지 못하는 경우이다. 메모리가 100%가 아닌 경우, 작은 Batch size가 문제일 수 있다. 연산 능력이 100%가 아닌 경우에는 모델의 계산 과정에서 CPU 병목이 원인일 수 있다.

## Batch size와 GPU 메모리의 관계는?

먼저 GPU 메모리란 GPU를 사용할 떄 한번에 가질 수 있는 메모리의 양이다. Batch size가 커지면 메모리의 양도 커진다. 그래서 필요한 메모리 양보다 GPU 메모리가 적을 경우에는 'out of memory'에러를 발생시키게 된다.

## Data parallelism이란?

Data parallelism은 학습해야할 데이터가 많은 상황에서 학습 속도를 높이기 위해 나온 분산 학습 방법이다. 예를 들자면 하나의 GPU가 1개의 Data를 학습해야 하는데 1분이 걸린다고 가정하면 1000개의 학습 데이터 Batch를 학습하려면 1000분이 걸리겠지만, 데이터를 100개씩 나눠 10개의 GPU에서 학습하면 100분이 걸릴 것이다. Data parallelism은 GPU의 수를 늘림을 통해 학습 데이터를 빨리 처리하여 시간을 단축하기 위한 전략이다.

## Loss Surface란?

모델을 훈련시킨다는 말은 비용 함수에서 파라미터를 업데이트하며 global minimum을 찾는 과정이다. Loss Surface란 global minimum을 찾아가는 과정을 시각화한 것으로, 모델을 이해하고 설계하는데 인사이트를 준다.

## Dropout이란?

Dropout은 노드들의 연결을 무작위로 끊는 방식으로, 하나의 노드가 너무 큰 가중치를 가져 다른 노드들의 학습을 방해하는 현상을 억제한다. 이를 통해 모델의 일반화 성능을 높이고 Overfitting을 방지하는 효과를 얻을 수 있다.

## Gradient clipping이란?

학습하고자 하는 모델이 RNN이나 DNN같은 비선형 목적 함수를 가지고 있을 때 미분값이 매우 크거나 작아지는 경향이 있다. 이러한 결과는 여태 진행했던 학습 Epoch가 무효화 될 수 있어 모델 학습시 loss nan 문제를 겪을 수 있다. Gradient clipping이란 Gradient의 최대 갯수를 제한하고, Gradient가 최대치를 넘게 되면 Gradient의 크기를 재조정하는 것이다. 이러한 Gradient Clipping은 최적화 알고리즘이 가야하는 방향은 그대로 유지하면서 업데이트되야하는 step의 크기(learning rate)를 자동으로 조정하게 된다.

## Gradient Accumulation이란?

미니 배치를 통해 구해진 Gradient를 n-step동안 Global Gradients에 누적시킨 후, 한번에 업데이트하는 방법이다. [관련 블로그](https://velog.io/@twinjuy/OOM%EB%A5%BC-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0-%EC%9C%84%ED%95%9C-Batch-Accumulation)

## Average Pooling과 Max Pooling의 차이점은?

Pooling은 Feature map에서 feature 수를 감소시키는 역할을 한다. Average Pooling은 kernel window에 해당하는 값들의 평균을 대표로, Max Pooling은 가장 큰 값을 대표로 feature 수를 감소시킨다. Max pooling은 kernel 영역 내에서 가장 두드러지는 값을 남기고, average는 영역 내의 모든 값을 고려하는 효과가 있다.

## Max Pooling의 장/단점은?

Max pooling의 장점은 데이터의 차원이 감소하여 신경망의 계산효율성이 좋아진다는 장점이 있다. 하지만 신경망이 깊어질수록 feature를 미세하게 보존할 수 없다는 단점이 있다.

## Fully Connected Layer의 기능은 무엇인가?

번역 그대로 완전 연결 된 계층이다. 모든 뉴런이 그 다음 층의 모든 뉴런과 연결된 상태를 말한다. 여기에 활성화 함수를 적용해서 이미지를 분류하는 기능을한다.

## Localization이란?

Object detection에서 검출한 객체의 위치 정보를 예측하는 것을 의미한다. 네트워크의 output vector에 좌표 정보를 출력하게끔 학습할 수 있다.

## Few-Shot Learning이란?

Few-Shot Learning은 쉽게 말하자면 훈련을 통해 class를 분류하는 것을 배우고, image가 들어왔을 때 어떤 것과 같은 class인지 구분하는 일을 하도록 하는 것이다. input image가 어떤 class에 속하는지를 배우는게 아닌, 어떤 class와 같은 class이냐를 배우는 것이다.

## Federated Learning이란?

Federated Learning이란 "연합 학습"으로써 다수의 로컬 클라이언트와 하나의 중앙 서버가 협력하여 데이터가 탈중앙화된 상황에서 글로벌 모델을 학습하는 기술이다. 여기서 로컬 클라이언트는 사물 인터넷 기기, 스마트 폰 등을 말한다. Federated Learning은 데이터 프라이버시 향상과 커뮤니케이션 효율성, 이 두 가지 장점때문에 굉장히 유용하다.

## self-supervised learning이란?

label이 없는 Untagged data를 기반으로 한 학습으로 자기 스스로 학습 데이터에 대한 분류를 수행하기 때문에 self가 붙는다. [관련링크](https://lifeisenjoyable.tistory.com/15)

## DataLoader에서 num_workers 파라미터는 대체 무엇일까?

num_workers는 멀티 프로세싱과 관련된 파라미터로, 머신러닝 학습을 좀 더 빠르게 진행하는데 사용하는 GPU는 기본적으로 CPU의 컨트롤을 받기 때문에 CPU의 성능도 GPU의 성능에 지대한 영향을 줄 수 있다. num_workers는 학습 도중 CPU의 작업을 몇 개의 코어를 사용해서 진행할지에 대한 설정 파라미터이다. [추천글](https://jybaek.tistory.com/799)

## torch.no_grad()의 의미는 무엇일까?

torch.no_grad() 함수는 gradient계산에서 context를 비활성화 해주는 역할을 한다고 한다. 그래서 이 함수를 사용해 줌으로써 필요한 메모리가 줄어들고 연산속도가 증가하게 된다.

## inductive biases란?

만나지 못한 상황을 해결하기 위해, 추가적인 가정을 활용해서 문제를 해결하는 방법이다. 예를들면 CNN은 지역성(Locality)를 공간적인 문제를 풀고, RNN은 순차성(Sequentiality)라는 가정으로 통해 시계열 문제를 해결한다.

## Early Stopping이란?

Epoch를 어떻게 설정해야하는가에 따른 딜레마가 있기 때문에 무조건 Epoch를 많이 돌린 후 특정 시점에서 멈추는 것이다. 그 특정시점을 정하는 방법은 validation set에서의 성능이 더이상 증가하지 않을 때 중지시키는 것이 일반적이다.

## Dliated Convolution

Dliated Convolution는 간단히 말하자면 기존 Convolution filter가 수용하는 픽셀 가이에 간격을 둔 형태이다. 입력 픽셀 수는 동일하지만 더 넗은 범위에 대한 입력을 수용할 수 있게된다. Segmentation 분야에서 많이쓰인다.